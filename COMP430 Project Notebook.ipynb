{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c9eccd3b",
      "metadata": {
        "id": "c9eccd3b"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "80d14f29",
      "metadata": {
        "id": "80d14f29",
        "outputId": "f968423a-ac1a-4bfb-fa6e-f2f53f255091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.5.1+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from opacus import PrivacyEngine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "1K6uC4x6gQen"
      },
      "id": "1K6uC4x6gQen"
    },
    {
      "cell_type": "markdown",
      "id": "5871106a",
      "metadata": {
        "id": "5871106a"
      },
      "source": [
        "Read the data. The read_excel function deemed the file as a .csv file instead of an Excel file so I had to use the\n",
        "read_csv function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7215c9f",
      "metadata": {
        "id": "f7215c9f"
      },
      "outputs": [],
      "source": [
        "file_path = \"./healthcare-dataset-stroke-data.csv\"\n",
        "stroke_data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc59394",
      "metadata": {
        "id": "3bc59394"
      },
      "source": [
        "The target column of our data is the \"stroke\" column, which is 0 if the subject hasn't had a stroke, and 1 if the patient has had a stroke. I dropped the data points that include NaN values, which dropped around 200 data points.\n",
        "\n",
        "Finally, I dropped the id feature as it is completely useless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd8a806",
      "metadata": {
        "id": "abd8a806"
      },
      "outputs": [],
      "source": [
        "target_column = \"stroke\"\n",
        "stroke_data.dropna(inplace= True)\n",
        "stroke_data = stroke_data.drop(columns=\"id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e96e15",
      "metadata": {
        "id": "39e96e15"
      },
      "source": [
        "Find the numerical and categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee84dbc5",
      "metadata": {
        "id": "ee84dbc5"
      },
      "outputs": [],
      "source": [
        "numerical_columns = stroke_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "categorical_columns = stroke_data.select_dtypes(include=[\"object\"]).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6faadb",
      "metadata": {
        "id": "9d6faadb"
      },
      "source": [
        "Encode the categorical data using one hot encoding with the pandas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb73893",
      "metadata": {
        "scrolled": true,
        "id": "1eb73893",
        "outputId": "718b75cf-7237-4309-e0b8-921215b42249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    age  hypertension  heart_disease  avg_glucose_level   bmi  stroke  \\\n",
            "0  67.0             0              1             228.69  36.6       1   \n",
            "2  80.0             0              1             105.92  32.5       1   \n",
            "3  49.0             0              0             171.23  34.4       1   \n",
            "4  79.0             1              0             174.12  24.0       1   \n",
            "5  81.0             0              0             186.21  29.0       1   \n",
            "\n",
            "   gender_Male  gender_Other  ever_married_Yes  work_type_Never_worked  \\\n",
            "0         True         False              True                   False   \n",
            "2         True         False              True                   False   \n",
            "3        False         False              True                   False   \n",
            "4        False         False              True                   False   \n",
            "5         True         False              True                   False   \n",
            "\n",
            "   work_type_Private  work_type_Self-employed  work_type_children  \\\n",
            "0               True                    False               False   \n",
            "2               True                    False               False   \n",
            "3               True                    False               False   \n",
            "4              False                     True               False   \n",
            "5               True                    False               False   \n",
            "\n",
            "   Residence_type_Urban  smoking_status_formerly smoked  \\\n",
            "0                  True                            True   \n",
            "2                 False                           False   \n",
            "3                  True                           False   \n",
            "4                 False                           False   \n",
            "5                  True                            True   \n",
            "\n",
            "   smoking_status_never smoked  smoking_status_smokes  \n",
            "0                        False                  False  \n",
            "2                         True                  False  \n",
            "3                        False                   True  \n",
            "4                         True                  False  \n",
            "5                        False                  False  \n",
            "               age  hypertension  heart_disease  avg_glucose_level  \\\n",
            "count  4909.000000   4909.000000    4909.000000        4909.000000   \n",
            "mean     42.865374      0.091872       0.049501         105.305150   \n",
            "std      22.555115      0.288875       0.216934          44.424341   \n",
            "min       0.080000      0.000000       0.000000          55.120000   \n",
            "25%      25.000000      0.000000       0.000000          77.070000   \n",
            "50%      44.000000      0.000000       0.000000          91.680000   \n",
            "75%      60.000000      0.000000       0.000000         113.570000   \n",
            "max      82.000000      1.000000       1.000000         271.740000   \n",
            "\n",
            "               bmi       stroke  \n",
            "count  4909.000000  4909.000000  \n",
            "mean     28.893237     0.042575  \n",
            "std       7.854067     0.201917  \n",
            "min      10.300000     0.000000  \n",
            "25%      23.500000     0.000000  \n",
            "50%      28.100000     0.000000  \n",
            "75%      33.100000     0.000000  \n",
            "max      97.600000     1.000000  \n"
          ]
        }
      ],
      "source": [
        "stroke_data_encoded = pd.get_dummies(stroke_data, columns= categorical_columns, drop_first= True)\n",
        "\n",
        "print(stroke_data_encoded.head())\n",
        "print(stroke_data_encoded.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the SMOTE strategy to oversample the minority class, so that the data is more heterogeneous."
      ],
      "metadata": {
        "id": "FqS291E-6aNP"
      },
      "id": "FqS291E-6aNP"
    },
    {
      "cell_type": "markdown",
      "id": "3f01d9ab",
      "metadata": {
        "id": "3f01d9ab"
      },
      "source": [
        "Prepare the data matrix and the label vector, plus the training and testing sets. The train-test split is 80%-20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc19fac2",
      "metadata": {
        "id": "fc19fac2"
      },
      "outputs": [],
      "source": [
        "X = stroke_data_encoded.drop(columns= target_column)\n",
        "y = stroke_data_encoded[target_column]\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "X_balanced[\"age\"] = X_balanced[\"age\"].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size= 0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data for PyTorch."
      ],
      "metadata": {
        "id": "fe8tmT2gNdPP"
      },
      "id": "fe8tmT2gNdPP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle object columns in X_train and X_test\n",
        "X_train_torch = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "X_test_torch = X_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values\n",
        "X_train_torch.fillna(0, inplace=True)\n",
        "X_test_torch.fillna(0, inplace=True)\n",
        "\n",
        "# Explicitly convert all columns to a supported dtype before creating the tensor\n",
        "X_train_torch = X_train_torch.astype(np.float32) # Convert all columns to float32\n",
        "X_test_torch = X_test_torch.astype(np.float32)   # Convert all columns to float32\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train_torch.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_torch.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "BAyYg8345wK_"
      },
      "id": "BAyYg8345wK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with Zero Privacy Using SciKit"
      ],
      "metadata": {
        "id": "Lg4WOHs6ghWR"
      },
      "id": "Lg4WOHs6ghWR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a simple logistic regression model for now, using ALL the data points with zero DP and zero FL."
      ],
      "metadata": {
        "id": "VHz3SDN0JxeB"
      },
      "id": "VHz3SDN0JxeB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90339602",
      "metadata": {
        "id": "90339602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "e4056f9d-b496-42d9-f078-7b271d6c04bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = LogisticRegression(max_iter= 1000)\n",
        "\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e95765",
      "metadata": {
        "id": "96e95765"
      },
      "source": [
        "Check the model's accuracy, recall, precision and F1 score on the test data and generate the model's confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4184c42",
      "metadata": {
        "id": "a4184c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "aa5fe3fb-64a6-44d4-9247-e33f9514d565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8734042553191489\n",
            "Recall:  0.872651356993737\n",
            "Precision:  0.8781512605042017\n",
            "F1 Score:  0.875392670157068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7e0d8faa4710>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Recall: \", recall_score(y_test, y_pred))\n",
        "print(\"Precision: \", precision_score(y_test, y_pred))\n",
        "print(\"F1 Score: \", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing DP on the Logistic Regression Model (PyTorch)**"
      ],
      "metadata": {
        "id": "oIAAm746cYZV"
      },
      "id": "oIAAm746cYZV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining logistic regression for PyTorch"
      ],
      "metadata": {
        "id": "AHPfXztVMm3p"
      },
      "id": "AHPfXztVMm3p"
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "model = LogisticRegressionModel(input_dim)"
      ],
      "metadata": {
        "id": "uVKpaxFW51Nb"
      },
      "id": "uVKpaxFW51Nb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training function for the logistic regression model."
      ],
      "metadata": {
        "id": "JIMZn_pEcisn"
      },
      "id": "JIMZn_pEcisn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the optimizer and Privacy Engine\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Training loop\n",
        "def train_logistic_regression(model, optimizer, criterion, train_loader, target_epsilon, epochs=5):\n",
        "    privacy_engine = PrivacyEngine()\n",
        "\n",
        "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    max_grad_norm=1.0,\n",
        "    epochs=5,\n",
        "    target_epsilon = target_epsilon,\n",
        "    target_delta = 1e-5,\n",
        ")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch).squeeze()\n",
        "            loss = criterion(y_pred, y_batch.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    epsilon = privacy_engine.get_epsilon(1e-5)\n",
        "    print(f\"Privacy budget spent: ε = {epsilon:.2f}\\n\")\n",
        "    accuracy, precision, recall, f1 = evaluate_logistic_regression(model, test_loader)\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\\n\")"
      ],
      "metadata": {
        "id": "gwcCJDckJVqL"
      },
      "id": "gwcCJDckJVqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to evaluate the performance of the LR model."
      ],
      "metadata": {
        "id": "wamfzafHe5JR"
      },
      "id": "wamfzafHe5JR"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_logistic_regression(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            predictions = (outputs > 0.5).int()\n",
        "            y_true.extend(y_batch.tolist())\n",
        "            y_pred.extend(predictions.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "A4I-7OSIe5bL"
      },
      "id": "A4I-7OSIe5bL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains and evaluates the LR model's performance with DP using epsilon values from 0.1 to 0.9."
      ],
      "metadata": {
        "id": "hwCLotWgfL_x"
      },
      "id": "hwCLotWgfL_x"
    },
    {
      "cell_type": "code",
      "source": [
        "for epsilon in range(1,10):\n",
        "  # Initialize model\n",
        "    epsilon /= 10\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = LogisticRegressionModel(input_dim)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    model = LogisticRegressionModel(X_train_tensor.shape[1])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    train_logistic_regression(model, optimizer, criterion, train_loader,epsilon, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7KvR_cjfMU9",
        "outputId": "ab920bdf-1808-43bd-e336-029344949a0d"
      },
      "id": "Y7KvR_cjfMU9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.09\n",
            "\n",
            "Accuracy: 0.7229, Precision: 0.7398, Recall: 0.7035, F1 Score: 0.7212\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.20\n",
            "\n",
            "Accuracy: 0.7218, Precision: 0.7235, Recall: 0.7349, F1 Score: 0.7292\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.30\n",
            "\n",
            "Accuracy: 0.7207, Precision: 0.6956, Recall: 0.8038, F1 Score: 0.7458\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.39\n",
            "\n",
            "Accuracy: 0.7064, Precision: 0.7411, Recall: 0.6514, F1 Score: 0.6933\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.49\n",
            "\n",
            "Accuracy: 0.7117, Precision: 0.7653, Recall: 0.6263, F1 Score: 0.6889\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.60\n",
            "\n",
            "Accuracy: 0.7069, Precision: 0.7619, Recall: 0.6180, F1 Score: 0.6824\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.70\n",
            "\n",
            "Accuracy: 0.7043, Precision: 0.7264, Recall: 0.6733, F1 Score: 0.6988\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.80\n",
            "\n",
            "Accuracy: 0.7027, Precision: 0.6962, Recall: 0.7390, F1 Score: 0.7170\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privacy budget spent: ε = 0.90\n",
            "\n",
            "Accuracy: 0.7176, Precision: 0.7434, Recall: 0.6806, F1 Score: 0.7106\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separation of Hospitals"
      ],
      "metadata": {
        "id": "2yzuBsp0fXpO"
      },
      "id": "2yzuBsp0fXpO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to separate the data into three different hospital sets, ensuring that it is natural, meaning the number of stroke patients should remain the minority."
      ],
      "metadata": {
        "id": "k1vLJgP2ffUv"
      },
      "id": "k1vLJgP2ffUv"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35f8805b",
      "metadata": {
        "id": "35f8805b"
      },
      "outputs": [],
      "source": [
        "def create_hospital_data(file_path, hospital_configs):\n",
        "    \"\"\"\n",
        "    Create hospital datasets with specified sizes and stroke rates\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the input CSV file\n",
        "        hospital_configs: List of tuples containing (hospital_size, stroke_rate) for each hospital\n",
        "\n",
        "    Returns:\n",
        "        List of pandas DataFrames for each hospital\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    stroke_data = data[data['stroke'] == 1]\n",
        "    non_stroke_data = data[data['stroke'] == 0]\n",
        "\n",
        "    hospital_datasets = []\n",
        "\n",
        "    for hospital_size, stroke_rate in hospital_configs:\n",
        "        stroke_count = int(hospital_size * stroke_rate)\n",
        "        non_stroke_count = hospital_size - stroke_count\n",
        "\n",
        "        hospital_stroke = stroke_data.sample(n=stroke_count, random_state=42)\n",
        "        hospital_non_stroke = non_stroke_data.sample(n=non_stroke_count, random_state=42)\n",
        "\n",
        "        hospital_data = pd.concat([hospital_stroke, hospital_non_stroke])\n",
        "        hospital_datasets.append(hospital_data)\n",
        "\n",
        "    return hospital_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hospital_configs = [\n",
        "    (3000, 0.20),  # Hospital 1: size=3000, stroke_rate=0.20\n",
        "    (2700, 0.15),  # Hospital 2: size=2700, stroke_rate=0.15\n",
        "    (3200, 0.10)   # Hospital 3: size=3200, stroke_rate=0.10\n",
        "]\n",
        "\n",
        "file_path = \"./balanced_stroke_data.csv\"\n",
        "hospital_data = create_hospital_data(file_path, hospital_configs)\n",
        "\n",
        "for i, data in enumerate(hospital_data, 1):\n",
        "    print(f\"Hospital {i}\\n\", data.describe())\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "YNygkQNgdZxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c8ed03-8d73-4391-da2e-e1ecba377836"
      },
      "id": "YNygkQNgdZxK",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hospital 1\n",
            "                age  hypertension  heart_disease  avg_glucose_level  \\\n",
            "count  3000.000000   3000.000000    3000.000000        3000.000000   \n",
            "mean     47.253333      0.082667       0.046667         110.227846   \n",
            "std      22.840426      0.275424       0.210959          49.034773   \n",
            "min       0.000000      0.000000       0.000000          55.120000   \n",
            "25%      30.000000      0.000000       0.000000          77.482500   \n",
            "50%      50.000000      0.000000       0.000000          93.565000   \n",
            "75%      66.000000      0.000000       0.000000         120.007500   \n",
            "max      82.000000      1.000000       1.000000         270.743390   \n",
            "\n",
            "               bmi       stroke  \n",
            "count  3000.000000  3000.000000  \n",
            "mean     29.192880     0.200000  \n",
            "std       7.548688     0.400067  \n",
            "min      11.500000     0.000000  \n",
            "25%      24.200000     0.000000  \n",
            "50%      28.400000     0.000000  \n",
            "75%      33.178893     0.000000  \n",
            "max      92.000000     1.000000  \n",
            "\n",
            "\n",
            "\n",
            "Hospital 2\n",
            "                age  hypertension  heart_disease  avg_glucose_level  \\\n",
            "count  2700.000000   2700.000000    2700.000000        2700.000000   \n",
            "mean     45.986296      0.081481       0.045926         108.610972   \n",
            "std      22.653490      0.273624       0.209363          47.729467   \n",
            "min       0.000000      0.000000       0.000000          55.120000   \n",
            "25%      28.000000      0.000000       0.000000          77.435000   \n",
            "50%      49.000000      0.000000       0.000000          92.713601   \n",
            "75%      64.000000      0.000000       0.000000         117.317753   \n",
            "max      82.000000      1.000000       1.000000         267.382950   \n",
            "\n",
            "               bmi       stroke  \n",
            "count  2700.000000  2700.000000  \n",
            "mean     29.129314     0.150000  \n",
            "std       7.662218     0.357138  \n",
            "min      11.500000     0.000000  \n",
            "25%      24.000000     0.000000  \n",
            "50%      28.400000     0.000000  \n",
            "75%      33.189541     0.000000  \n",
            "max      92.000000     1.000000  \n",
            "\n",
            "\n",
            "\n",
            "Hospital 3\n",
            "                age  hypertension  heart_disease  avg_glucose_level  \\\n",
            "count  3200.000000   3200.000000    3200.000000        3200.000000   \n",
            "mean     44.402813      0.079062       0.043437         107.374046   \n",
            "std      22.530364      0.269878       0.203872          46.612760   \n",
            "min       0.000000      0.000000       0.000000          55.120000   \n",
            "25%      27.000000      0.000000       0.000000          77.160000   \n",
            "50%      46.000000      0.000000       0.000000          92.205000   \n",
            "75%      62.000000      0.000000       0.000000         115.997500   \n",
            "max      82.000000      1.000000       1.000000         267.600000   \n",
            "\n",
            "               bmi       stroke  \n",
            "count  3200.000000  3200.000000  \n",
            "mean     29.027935     0.100000  \n",
            "std       7.731435     0.300047  \n",
            "min      10.300000     0.000000  \n",
            "25%      23.800000     0.000000  \n",
            "50%      28.300000     0.000000  \n",
            "75%      33.100000     0.000000  \n",
            "max      92.000000     1.000000  \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export each hospital dataset\n",
        "for i, data in enumerate(hospital_data, 1):\n",
        "    data.to_csv(f'hospital_{i}_data.csv', index=False)\n",
        "    print(f\"Exported Hospital {i} data to hospital_{i}_data.csv\")"
      ],
      "metadata": {
        "id": "MmTr_oYQeeBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d78d25-ee26-47c7-db0d-224cc678958c"
      },
      "id": "MmTr_oYQeeBN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported Hospital 1 data to hospital_1_data.csv\n",
            "Exported Hospital 2 data to hospital_2_data.csv\n",
            "Exported Hospital 3 data to hospital_3_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 3 hospitals are extracted and turned into tensors in PyTorch."
      ],
      "metadata": {
        "id": "EMwXExGdAdcj"
      },
      "id": "EMwXExGdAdcj"
    },
    {
      "cell_type": "code",
      "source": [
        "hospitals_in_torch=[]\n",
        "for hospital in hospital_data:\n",
        "  labels = hospital['stroke']\n",
        "  features = hospital.drop(columns=['stroke'])\n",
        "\n",
        "  X = torch.tensor(features.astype(np.float32).values, dtype=torch.float32)\n",
        "  y = torch.tensor(labels.values, dtype=torch.long)\n",
        "  dataset = TensorDataset(X, y)\n",
        "  hospitals_in_torch.append(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "770CDn9N_aL0"
      },
      "id": "770CDn9N_aL0",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to train a neural network using Differential Privacy."
      ],
      "metadata": {
        "id": "jnlNGwe0AXqi"
      },
      "id": "jnlNGwe0AXqi"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_with_dp(local_dataset, model, optimizer, criterion, epochs, target_epsilon):\n",
        "    dataloader = DataLoader(local_dataset, batch_size=32, shuffle=True)\n",
        "    privacy_engine = PrivacyEngine()\n",
        "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=dataloader,\n",
        "        max_grad_norm=1.0,\n",
        "        epochs=epochs,\n",
        "        target_epsilon=target_epsilon,\n",
        "        target_delta= 1e-5,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate epsilon, accuracy, precision, recall, and F1 Score at the end of each epoch\n",
        "        epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
        "        accuracy, precision, recall, f1 = evaluate_model(model, local_dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"Loss: {epoch_loss / len(dataloader):.6f}\")\n",
        "        print(f\"ε = {epsilon:.2f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\\n\")"
      ],
      "metadata": {
        "id": "OFFPH_9lAwfB"
      },
      "id": "OFFPH_9lAwfB",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to evaluate the neural network."
      ],
      "metadata": {
        "id": "bLocxTXTA48s"
      },
      "id": "bLocxTXTA48s"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    dataloader = DataLoader(dataset, batch_size=32)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "JnT26So4BB5p"
      },
      "id": "JnT26So4BB5p",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 models, corresponding to the local models of each hospital, are trained and evaluated, using epsilon values from 0.1 to 0.9."
      ],
      "metadata": {
        "id": "zwYkp3wRBG6o"
      },
      "id": "zwYkp3wRBG6o"
    },
    {
      "cell_type": "code",
      "source": [
        "for epsilon in range(1, 10):\n",
        "    epsilon /= 10\n",
        "    for i in range(3):\n",
        "        model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(X.shape[1], 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 2)\n",
        "        ).to(device)\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        print(f\"\\nTraining model {i + 1}\")\n",
        "        print(f\"Target Epsilon: {epsilon:.1f}\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            train_with_dp(hospitals_in_torch[i], model, optimizer, criterion, epochs=5, target_epsilon=epsilon)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error training model {i + 1} with epsilon {epsilon:.1f}: {e}\")\n",
        "            print(\"Consider increasing target_delta or reducing target_epsilon.\")\n",
        "            continue  # Move to the next model or epsilon value"
      ],
      "metadata": {
        "id": "Qugc2rrZBKAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40584dc-7ea6-4079-cf83-2b519ef178ec"
      },
      "id": "Qugc2rrZBKAQ",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.050827\n",
            "ε = 0.04, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.938514\n",
            "ε = 0.06, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.790480\n",
            "ε = 0.07, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.860418\n",
            "ε = 0.08, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.768847\n",
            "ε = 0.09, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.171004\n",
            "ε = 0.05, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 2.129210\n",
            "ε = 0.06, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 2.009850\n",
            "ε = 0.07, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.762849\n",
            "ε = 0.09, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.769933\n",
            "ε = 0.10, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.296542\n",
            "ε = 0.05, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.302591\n",
            "ε = 0.06, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.126546\n",
            "ε = 0.07, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.092114\n",
            "ε = 0.08, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.277090\n",
            "ε = 0.09, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.180747\n",
            "ε = 0.09, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 2.249675\n",
            "ε = 0.12, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 2.189816\n",
            "ε = 0.15, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.978102\n",
            "ε = 0.17, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.934803\n",
            "ε = 0.19, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.689628\n",
            "ε = 0.09, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.809923\n",
            "ε = 0.12, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.785070\n",
            "ε = 0.15, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.796401\n",
            "ε = 0.17, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.653434\n",
            "ε = 0.20, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.394697\n",
            "ε = 0.09, Accuracy: 89.97%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.528431\n",
            "ε = 0.12, Accuracy: 89.97%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.343021\n",
            "ε = 0.15, Accuracy: 89.97%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.435536\n",
            "ε = 0.17, Accuracy: 89.97%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.368537\n",
            "ε = 0.19, Accuracy: 89.97%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 3.406571\n",
            "ε = 0.13, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 2.163864\n",
            "ε = 0.18, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 2.068922\n",
            "ε = 0.23, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.791675\n",
            "ε = 0.26, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.639975\n",
            "ε = 0.29, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.342228\n",
            "ε = 0.13, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.890021\n",
            "ε = 0.18, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.720152\n",
            "ε = 0.23, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.744804\n",
            "ε = 0.26, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.630473\n",
            "ε = 0.29, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.195496\n",
            "ε = 0.13, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.163077\n",
            "ε = 0.18, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.022280\n",
            "ε = 0.23, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.214621\n",
            "ε = 0.26, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.065325\n",
            "ε = 0.29, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.239240\n",
            "ε = 0.18, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.911866\n",
            "ε = 0.25, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.937425\n",
            "ε = 0.30, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.678895\n",
            "ε = 0.35, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.617855\n",
            "ε = 0.39, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.998720\n",
            "ε = 0.18, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.629551\n",
            "ε = 0.25, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.540867\n",
            "ε = 0.31, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.463136\n",
            "ε = 0.35, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.387216\n",
            "ε = 0.40, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.122695\n",
            "ε = 0.18, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 0.940014\n",
            "ε = 0.25, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 0.816062\n",
            "ε = 0.31, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 0.910586\n",
            "ε = 0.35, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 0.944294\n",
            "ε = 0.40, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.182275\n",
            "ε = 0.23, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.949700\n",
            "ε = 0.31, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.819626\n",
            "ε = 0.38, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.834103\n",
            "ε = 0.44, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.818142\n",
            "ε = 0.50, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.524937\n",
            "ε = 0.23, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.336687\n",
            "ε = 0.32, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.233602\n",
            "ε = 0.39, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.319413\n",
            "ε = 0.45, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.313919\n",
            "ε = 0.50, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.353787\n",
            "ε = 0.22, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.297708\n",
            "ε = 0.31, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.277331\n",
            "ε = 0.38, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.150881\n",
            "ε = 0.44, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.227556\n",
            "ε = 0.49, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 3.545409\n",
            "ε = 0.27, Accuracy: 79.87%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 2.655843\n",
            "ε = 0.38, Accuracy: 79.97%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 2.341613\n",
            "ε = 0.46, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 2.116241\n",
            "ε = 0.53, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.854466\n",
            "ε = 0.59, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.345217\n",
            "ε = 0.27, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.471332\n",
            "ε = 0.38, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.436928\n",
            "ε = 0.46, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.425101\n",
            "ε = 0.53, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.332602\n",
            "ε = 0.59, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 0.959462\n",
            "ε = 0.27, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 0.868067\n",
            "ε = 0.38, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 0.885542\n",
            "ε = 0.46, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 0.820706\n",
            "ε = 0.53, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 0.881881\n",
            "ε = 0.59, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.005260\n",
            "ε = 0.33, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.763343\n",
            "ε = 0.45, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.790692\n",
            "ε = 0.54, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.726134\n",
            "ε = 0.63, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.620745\n",
            "ε = 0.70, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.060941\n",
            "ε = 0.33, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.577027\n",
            "ε = 0.45, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.452461\n",
            "ε = 0.54, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.505275\n",
            "ε = 0.63, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.356293\n",
            "ε = 0.70, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.886130\n",
            "ε = 0.32, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.173972\n",
            "ε = 0.45, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.045734\n",
            "ε = 0.54, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.101793\n",
            "ε = 0.62, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 0.913716\n",
            "ε = 0.70, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.656688\n",
            "ε = 0.38, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.895202\n",
            "ε = 0.51, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.714402\n",
            "ε = 0.62, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.684971\n",
            "ε = 0.71, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.518769\n",
            "ε = 0.79, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.786068\n",
            "ε = 0.38, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.499354\n",
            "ε = 0.51, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.532872\n",
            "ε = 0.62, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.369705\n",
            "ε = 0.71, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.489884\n",
            "ε = 0.80, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.050669\n",
            "ε = 0.38, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.012588\n",
            "ε = 0.51, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 0.996864\n",
            "ε = 0.62, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 0.971793\n",
            "ε = 0.71, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 0.943731\n",
            "ε = 0.79, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n",
            "\n",
            "Training model 1\n",
            "Target Epsilon: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.390270\n",
            "ε = 0.44, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.819955\n",
            "ε = 0.59, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.744141\n",
            "ε = 0.70, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.735079\n",
            "ε = 0.81, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.578196\n",
            "ε = 0.90, Accuracy: 80.00%, Precision: 0.64, Recall: 0.80, F1: 0.71\n",
            "\n",
            "\n",
            "Training model 2\n",
            "Target Epsilon: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 1.746732\n",
            "ε = 0.43, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 1.528840\n",
            "ε = 0.58, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.456666\n",
            "ε = 0.70, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.480958\n",
            "ε = 0.80, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 1.404838\n",
            "ε = 0.89, Accuracy: 85.00%, Precision: 0.72, Recall: 0.85, F1: 0.78\n",
            "\n",
            "\n",
            "Training model 3\n",
            "Target Epsilon: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Loss: 2.248628\n",
            "ε = 0.44, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "Loss: 0.993409\n",
            "ε = 0.59, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "Loss: 1.029484\n",
            "ε = 0.70, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5\n",
            "Loss: 1.020574\n",
            "ε = 0.80, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n",
            "Loss: 0.954313\n",
            "ε = 0.90, Accuracy: 90.00%, Precision: 0.81, Recall: 0.90, F1: 0.85\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application of Federated Learning with Flower"
      ],
      "metadata": {
        "id": "hn3e_yEJdT5k"
      },
      "id": "hn3e_yEJdT5k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required Flower libraries."
      ],
      "metadata": {
        "id": "104wo732dZ4u"
      },
      "id": "104wo732dZ4u"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr\n",
        "!pip install -U \"flwr[simulation]\""
      ],
      "metadata": {
        "id": "d4eiVwgmdb2Z",
        "outputId": "d4a64307-b2d4-4fc6-c13b-905c68b0bf78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d4eiVwgmdb2Z",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr\n",
            "  Downloading flwr-1.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting cryptography<43.0.0,>=42.0.4 (from flwr)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting grpcio!=1.64.2,<2.0.0,<=1.64.3,>=1.60.0 (from flwr)\n",
            "  Downloading grpcio-1.64.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.26.4)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.5)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.14.0-py3-none-any.whl (523 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.6/523.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, tomli, pycryptodome, pathspec, iterators, grpcio, cryptography, typer, flwr\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.69.0\n",
            "    Uninstalling grpcio-1.69.0:\n",
            "      Successfully uninstalled grpcio-1.69.0\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.1\n",
            "    Uninstalling typer-0.15.1:\n",
            "      Successfully uninstalled typer-0.15.1\n",
            "Successfully installed cryptography-42.0.8 flwr-1.14.0 grpcio-1.64.3 iterators-0.0.2 pathspec-0.12.1 pycryptodome-3.21.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.14.0)\n",
            "Requirement already satisfied: cryptography<43.0.0,>=42.0.4 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (42.0.8)\n",
            "Requirement already satisfied: grpcio!=1.64.2,<2.0.0,<=1.64.3,>=1.60.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.64.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.26.4)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.25.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.5)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.21.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Collecting ray==2.10.0 (from flwr[simulation])\n",
            "  Downloading ray-2.10.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.10.0->flwr[simulation]) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<43.0.0,>=42.0.4->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.10.0->flwr[simulation]) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.10.0->flwr[simulation]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.10.0->flwr[simulation]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.10.0->flwr[simulation]) (0.22.3)\n",
            "Downloading ray-2.10.0-cp311-cp311-manylinux2014_x86_64.whl (65.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flwr\n",
        "from flwr import *\n",
        "from flwr.common import *\n",
        "from flwr.client import *\n",
        "from flwr.server import *\n",
        "from collections import OrderedDict\n",
        "from flwr.server.strategy import FedAvg\n",
        "from flwr.simulation import run_simulation"
      ],
      "metadata": {
        "id": "BPknZdOKddYj"
      },
      "id": "BPknZdOKddYj",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added the required model fit evaluation functions for the FedAvg method."
      ],
      "metadata": {
        "id": "7k265Ft0dfP6"
      },
      "id": "7k265Ft0dfP6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "def weighted_average_fit(metrics):\n",
        "  total_samples = sum([num_samples for num_samples, _ in metrics])\n",
        "  if total_samples == 0:\n",
        "    return {}\n",
        "  weighted_loss = sum(num_samples * metric.get(\"loss\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  return {\"loss\" : weighted_loss}\n",
        "\n",
        "def weighted_average_evaluate(metrics):\n",
        "  total_samples = sum([num_samples for num_samples, _ in metrics])\n",
        "  if total_samples == 0:\n",
        "    return {}\n",
        "  weighted_loss = sum(num_samples * metric.get(\"loss\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  weighted_accuracy = sum(num_samples * metric.get(\"accuracy\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  weighted_precision = sum(num_samples * metric.get(\"precision\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  weighted_recall = sum(num_samples * metric.get(\"recall\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  weighted_f1 = sum(num_samples * metric.get(\"f1\", 0) for num_samples, metric in metrics) / total_samples\n",
        "  return {\n",
        "      \"loss\": weighted_loss,\n",
        "      \"accuracy\": weighted_accuracy,\n",
        "      \"precision\": weighted_precision,\n",
        "      \"recall\": weighted_recall,\n",
        "      \"f1\": weighted_f1\n",
        "  }"
      ],
      "metadata": {
        "id": "Fa7c-23sdlAT",
        "outputId": "4fee5378-e1a1-4e27-9ddb-a6f16da487ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Fa7c-23sdlAT",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented the FlowerClient class and its corresponding functions for FL."
      ],
      "metadata": {
        "id": "0Dq_QOyAdnhV"
      },
      "id": "0Dq_QOyAdnhV"
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(flwr.client.NumPyClient):\n",
        "    def __init__(self, model, train_dataset):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.precision = torchmetrics.Precision(task=\"binary\").to(self.device)\n",
        "        self.recall = torchmetrics.Recall(task=\"binary\").to(self.device)\n",
        "        self.f1 = torchmetrics.F1Score(task=\"binary\").to(self.device)\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.tensor(v, device=self.device) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        train_loader = DataLoader(self.train_dataset, batch_size=32, shuffle=True)\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        cumulative_loss = 0\n",
        "\n",
        "        for _ in range(5):  # Local epochs\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                X, y = batch\n",
        "                X, y = X.to(self.device), y.to(self.device)\n",
        "                outputs = self.model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                cumulative_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        avg_loss = cumulative_loss / len(train_loader)\n",
        "        return self.get_parameters(config={}), len(self.train_dataset), {\"loss\": avg_loss}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        test_loader = DataLoader(self.train_dataset, batch_size=32)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "        total = 0\n",
        "\n",
        "        # Reset metrics\n",
        "        self.precision.reset()\n",
        "        self.recall.reset()\n",
        "        self.f1.reset()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                X, y = batch\n",
        "                X, y = X.to(self.device), y.to(self.device)\n",
        "                outputs = self.model(X)\n",
        "                loss += criterion(outputs, y).item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += y.size(0)\n",
        "                accuracy += (predicted == y).sum().item()\n",
        "\n",
        "                # Update metrics\n",
        "                self.precision.update(predicted, y)\n",
        "                self.recall.update(predicted, y)\n",
        "                self.f1.update(predicted, y)\n",
        "\n",
        "        # Compute final metrics\n",
        "        precision = self.precision.compute().item()\n",
        "        recall = self.recall.compute().item()\n",
        "        f1 = self.f1.compute().item()\n",
        "\n",
        "        return loss, len(self.train_dataset), {\n",
        "            \"loss\": loss / len(test_loader),\n",
        "            \"accuracy\": accuracy / total,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "def client_fn(context: Context):\n",
        "    hospital_idx = 0 if context.node_id == '0' else 1 if context.node_id == '1' else 2\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(X.shape[1], 128),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(128, 2)\n",
        "    )\n",
        "    return FlowerClient(model, hospitals_in_torch[hospital_idx]).to_client()\n",
        "\n",
        "\n",
        "def server_fn(context: Context):\n",
        "    server_config = ServerConfig(num_rounds=3)\n",
        "    strategy = FedAvg(\n",
        "        fit_metrics_aggregation_fn=weighted_average_fit,\n",
        "        evaluate_metrics_aggregation_fn=weighted_average_evaluate,\n",
        "    )\n",
        "    return ServerAppComponents(\n",
        "        strategy=strategy,\n",
        "        config=server_config,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "tbiKpxpcdtcf"
      },
      "id": "tbiKpxpcdtcf",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests the FL based approach."
      ],
      "metadata": {
        "id": "Yqd_njWddudA"
      },
      "id": "Yqd_njWddudA"
    },
    {
      "cell_type": "code",
      "source": [
        "client_app = ClientApp(client_fn=client_fn)\n",
        "server_app = ServerApp(server_fn=server_fn)\n",
        "\n",
        "run_simulation(\n",
        "    server_app=server_app,\n",
        "    client_app=client_app,\n",
        "    num_supernodes=3,\n",
        "    backend_config={\"client_resources\": {\"num_cpus\": 2, \"num_gpus\": 1}}\n",
        ")"
      ],
      "metadata": {
        "id": "y1wt_mzTdyzy",
        "outputId": "fafbd823-ec29-460f-863e-ef7a5342ee60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y1wt_mzTdyzy",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:flwr:Asyncio event loop already running.\n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=5122)\u001b[0m 2025-01-24 14:26:53.000242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=5122)\u001b[0m 2025-01-24 14:26:53.019428: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=5122)\u001b[0m 2025-01-24 14:26:53.026306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=5122)\u001b[0m 2025-01-24 14:26:54.226874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 11.60s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 26.472128991037607\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 25.800009176135063\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 20.683937191963196\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, fit):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'loss': [(1, 1.8962991244904697),\n",
            "\u001b[92mINFO \u001b[0m:      \t          (2, 1.2304200594654928),\n",
            "\u001b[92mINFO \u001b[0m:      \t          (3, 1.0928140474235017)]}\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.9), (2, 0.9215625), (3, 0.939375)],\n",
            "\u001b[92mINFO \u001b[0m:      \t 'f1': [(1, 0.0), (2, 0.5163776278495789), (3, 0.6666666865348816)],\n",
            "\u001b[92mINFO \u001b[0m:      \t 'loss': [(1, 0.2647212899103761),\n",
            "\u001b[92mINFO \u001b[0m:      \t          (2, 0.25800009176135064),\n",
            "\u001b[92mINFO \u001b[0m:      \t          (3, 0.20683937191963195)],\n",
            "\u001b[92mINFO \u001b[0m:      \t 'precision': [(1, 0.0), (2, 0.6733668446540833), (3, 0.7404580116271973)],\n",
            "\u001b[92mINFO \u001b[0m:      \t 'recall': [(1, 0.0), (2, 0.41874998807907104), (3, 0.606249988079071)]}\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}